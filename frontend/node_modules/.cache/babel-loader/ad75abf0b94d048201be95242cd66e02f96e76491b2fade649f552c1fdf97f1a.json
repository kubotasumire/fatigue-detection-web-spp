{"ast":null,"code":"import { Engine } from \"../Engines/engine.js\";\nimport { Tools } from \"../Misc/tools.js\";\nimport { EngineStore } from \"../Engines/engineStore.js\";\n/**\n * Class used to work with sound analyzer using fast fourier transform (FFT)\n * @see https://doc.babylonjs.com/features/featuresDeepDive/audio/playingSoundsMusic\n */\nexport class Analyser {\n  /**\n   * Creates a new analyser\n   * @param scene defines hosting scene\n   */\n  constructor(scene) {\n    /**\n     * Gets or sets the smoothing\n     * @ignorenaming\n     */\n    this.SMOOTHING = 0.75;\n    /**\n     * Gets or sets the FFT table size\n     * @ignorenaming\n     */\n    this.FFT_SIZE = 512;\n    /**\n     * Gets or sets the bar graph amplitude\n     * @ignorenaming\n     */\n    this.BARGRAPHAMPLITUDE = 256;\n    /**\n     * Gets or sets the position of the debug canvas\n     * @ignorenaming\n     */\n    this.DEBUGCANVASPOS = {\n      x: 20,\n      y: 20\n    };\n    /**\n     * Gets or sets the debug canvas size\n     * @ignorenaming\n     */\n    this.DEBUGCANVASSIZE = {\n      width: 320,\n      height: 200\n    };\n    scene = scene || EngineStore.LastCreatedScene;\n    if (!scene) {\n      return;\n    }\n    this._scene = scene;\n    if (!Engine.audioEngine) {\n      Tools.Warn(\"No audio engine initialized, failed to create an audio analyser\");\n      return;\n    }\n    this._audioEngine = Engine.audioEngine;\n    if (this._audioEngine.canUseWebAudio && this._audioEngine.audioContext) {\n      this._webAudioAnalyser = this._audioEngine.audioContext.createAnalyser();\n      this._webAudioAnalyser.minDecibels = -140;\n      this._webAudioAnalyser.maxDecibels = 0;\n      this._byteFreqs = new Uint8Array(this._webAudioAnalyser.frequencyBinCount);\n      this._byteTime = new Uint8Array(this._webAudioAnalyser.frequencyBinCount);\n      this._floatFreqs = new Float32Array(this._webAudioAnalyser.frequencyBinCount);\n    }\n  }\n  /**\n   * Get the number of data values you will have to play with for the visualization\n   * @see https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/frequencyBinCount\n   * @returns a number\n   */\n  getFrequencyBinCount() {\n    if (this._audioEngine.canUseWebAudio) {\n      return this._webAudioAnalyser.frequencyBinCount;\n    } else {\n      return 0;\n    }\n  }\n  /**\n   * Gets the current frequency data as a byte array\n   * @see https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getByteFrequencyData\n   * @returns a Uint8Array\n   */\n  getByteFrequencyData() {\n    if (this._audioEngine.canUseWebAudio) {\n      this._webAudioAnalyser.smoothingTimeConstant = this.SMOOTHING;\n      this._webAudioAnalyser.fftSize = this.FFT_SIZE;\n      this._webAudioAnalyser.getByteFrequencyData(this._byteFreqs);\n    }\n    return this._byteFreqs;\n  }\n  /**\n   * Gets the current waveform as a byte array\n   * @see https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getByteTimeDomainData\n   * @returns a Uint8Array\n   */\n  getByteTimeDomainData() {\n    if (this._audioEngine.canUseWebAudio) {\n      this._webAudioAnalyser.smoothingTimeConstant = this.SMOOTHING;\n      this._webAudioAnalyser.fftSize = this.FFT_SIZE;\n      this._webAudioAnalyser.getByteTimeDomainData(this._byteTime);\n    }\n    return this._byteTime;\n  }\n  /**\n   * Gets the current frequency data as a float array\n   * @see https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getByteFrequencyData\n   * @returns a Float32Array\n   */\n  getFloatFrequencyData() {\n    if (this._audioEngine.canUseWebAudio) {\n      this._webAudioAnalyser.smoothingTimeConstant = this.SMOOTHING;\n      this._webAudioAnalyser.fftSize = this.FFT_SIZE;\n      this._webAudioAnalyser.getFloatFrequencyData(this._floatFreqs);\n    }\n    return this._floatFreqs;\n  }\n  /**\n   * Renders the debug canvas\n   */\n  drawDebugCanvas() {\n    if (this._audioEngine.canUseWebAudio) {\n      if (!this._debugCanvas) {\n        this._debugCanvas = document.createElement(\"canvas\");\n        this._debugCanvas.width = this.DEBUGCANVASSIZE.width;\n        this._debugCanvas.height = this.DEBUGCANVASSIZE.height;\n        this._debugCanvas.style.position = \"absolute\";\n        this._debugCanvas.style.top = this.DEBUGCANVASPOS.y + \"px\";\n        this._debugCanvas.style.left = this.DEBUGCANVASPOS.x + \"px\";\n        this._debugCanvasContext = this._debugCanvas.getContext(\"2d\");\n        document.body.appendChild(this._debugCanvas);\n        this._registerFunc = () => {\n          this.drawDebugCanvas();\n        };\n        this._scene.registerBeforeRender(this._registerFunc);\n      }\n      if (this._registerFunc && this._debugCanvasContext) {\n        const workingArray = this.getByteFrequencyData();\n        this._debugCanvasContext.fillStyle = \"rgb(0, 0, 0)\";\n        this._debugCanvasContext.fillRect(0, 0, this.DEBUGCANVASSIZE.width, this.DEBUGCANVASSIZE.height);\n        // Draw the frequency domain chart.\n        for (let i = 0; i < this.getFrequencyBinCount(); i++) {\n          const value = workingArray[i];\n          const percent = value / this.BARGRAPHAMPLITUDE;\n          const height = this.DEBUGCANVASSIZE.height * percent;\n          const offset = this.DEBUGCANVASSIZE.height - height - 1;\n          const barWidth = this.DEBUGCANVASSIZE.width / this.getFrequencyBinCount();\n          const hue = i / this.getFrequencyBinCount() * 360;\n          this._debugCanvasContext.fillStyle = \"hsl(\" + hue + \", 100%, 50%)\";\n          this._debugCanvasContext.fillRect(i * barWidth, offset, barWidth, height);\n        }\n      }\n    }\n  }\n  /**\n   * Stops rendering the debug canvas and removes it\n   */\n  stopDebugCanvas() {\n    if (this._debugCanvas) {\n      if (this._registerFunc) {\n        this._scene.unregisterBeforeRender(this._registerFunc);\n        this._registerFunc = null;\n      }\n      document.body.removeChild(this._debugCanvas);\n      this._debugCanvas = null;\n      this._debugCanvasContext = null;\n    }\n  }\n  /**\n   * Connects two audio nodes\n   * @param inputAudioNode defines first node to connect\n   * @param outputAudioNode defines second node to connect\n   */\n  connectAudioNodes(inputAudioNode, outputAudioNode) {\n    if (this._audioEngine.canUseWebAudio) {\n      inputAudioNode.connect(this._webAudioAnalyser);\n      this._webAudioAnalyser.connect(outputAudioNode);\n    }\n  }\n  /**\n   * Releases all associated resources\n   */\n  dispose() {\n    if (this._audioEngine.canUseWebAudio) {\n      this._webAudioAnalyser.disconnect();\n    }\n  }\n}\n//# sourceMappingURL=analyser.js.map","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}